{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>UUP sees possibility of voting Major out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Pubs targeted as curbs on smoking are extended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Papers reveal secret links with O'Neill cabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Domestic chaos as Italy takes EU presidency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Learning about the star to which we owe life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date headline_category  \\\n",
       "0      19960102              news   \n",
       "1      19960102              news   \n",
       "2      19960102              news   \n",
       "3      19960102              news   \n",
       "4      19960102              news   \n",
       "\n",
       "                                     headline_text  \n",
       "0         UUP sees possibility of voting Major out  \n",
       "1   Pubs targeted as curbs on smoking are extended  \n",
       "2  Papers reveal secret links with O'Neill cabinet  \n",
       "3      Domestic chaos as Italy takes EU presidency  \n",
       "4     Learning about the star to which we owe life  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_set = pd.read_csv('irishtimes-date-text.csv') # Reading in the data\n",
    "news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1425460 headline text\n"
     ]
    }
   ],
   "source": [
    "num = len(news_set['headline_text'])\n",
    "print(f'There are {num} headline text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9026 in the dataset\n"
     ]
    }
   ],
   "source": [
    "duplicates =news_set.duplicated().sum()\n",
    "print(f'There are {duplicates} in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This drops the duplicates\n",
    "news_set.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>UUP sees possibility of voting Major out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>Pubs targeted as curbs on smoking are extended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>Papers reveal secret links with O'Neill cabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>Domestic chaos as Italy takes EU presidency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>Learning about the star to which we owe life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  headline_category                                    headline_text\n",
       "0              news         UUP sees possibility of voting Major out\n",
       "1              news   Pubs targeted as curbs on smoking are extended\n",
       "2              news  Papers reveal secret links with O'Neill cabinet\n",
       "3              news      Domestic chaos as Italy takes EU presidency\n",
       "4              news     Learning about the star to which we owe life"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I personally will not be using the dates.\n",
    "news_set.drop(['publish_date'], axis = 1, inplace = True)\n",
    "news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, there are 1416434 headline text\n"
     ]
    }
   ],
   "source": [
    "num = len(news_set['headline_text'])\n",
    "print(f'Now, there are {num} headline text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the words containing numbers\n",
    "import re\n",
    "cleaned_text = []\n",
    "for i in news_set['headline_text']:\n",
    "    hold = re.sub('\\w*\\d\\w*', ' ', i)\n",
    "    cleaned_text.append(hold)\n",
    "news_set['cleaned_text'] = cleaned_text\n",
    "#news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting  to lower case\n",
    "lower_case_text =[]\n",
    "for i in news_set['cleaned_text']:\n",
    "    hold = i.lower()\n",
    "    lower_case_text.append(hold)\n",
    "\n",
    "news_set['lower_case_text'] =  lower_case_text\n",
    "#news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations and replacing them with white spaces.\n",
    "import string\n",
    "no_punctuation = []\n",
    "for i in news_set['lower_case_text']:\n",
    "    hold = re.sub('[%s]'%re.escape(string.punctuation), '',str(i))\n",
    "    no_punctuation.append(hold)\n",
    "    \n",
    "news_set['no_punctuation'] = no_punctuation\n",
    "#news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the words\n",
    "from nltk.tokenize import  word_tokenize\n",
    "\n",
    "tokenize= []\n",
    "for i in news_set['no_punctuation']:\n",
    "    hold = word_tokenize(i)\n",
    "    tokenize.append(hold)\n",
    "    \n",
    "news_set['tokenized_words'] = tokenize\n",
    "#news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "stop_func = lambda x: [word for word in x if word not in stopwords]\n",
    "news_set['removed_stopwords'] = news_set['tokenized_words'].apply(stop_func)\n",
    "#news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed = lambda x:[ ps.stem(i) for i in x]\n",
    "\n",
    "    \n",
    "news_set['stemmed_words'] =  news_set['removed_stopwords'].apply(stemmed)\n",
    "#news_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_format = []\n",
    "for i in news_set['stemmed_words']:\n",
    "    hold = ' '.join(i)\n",
    "    final_format.append(hold)\n",
    "    \n",
    "news_set['final_format'] = final_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lower_case_text</th>\n",
       "      <th>no_punctuation</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>stemmed_words</th>\n",
       "      <th>final_format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>UUP sees possibility of voting Major out</td>\n",
       "      <td>UUP sees possibility of voting Major out</td>\n",
       "      <td>uup sees possibility of voting major out</td>\n",
       "      <td>uup sees possibility of voting major out</td>\n",
       "      <td>[uup, sees, possibility, of, voting, major, out]</td>\n",
       "      <td>[uup, sees, possibility, voting, major]</td>\n",
       "      <td>[uup, see, possibl, vote, major]</td>\n",
       "      <td>uup see possibl vote major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>Pubs targeted as curbs on smoking are extended</td>\n",
       "      <td>Pubs targeted as curbs on smoking are extended</td>\n",
       "      <td>pubs targeted as curbs on smoking are extended</td>\n",
       "      <td>pubs targeted as curbs on smoking are extended</td>\n",
       "      <td>[pubs, targeted, as, curbs, on, smoking, are, ...</td>\n",
       "      <td>[pubs, targeted, curbs, smoking, extended]</td>\n",
       "      <td>[pub, target, curb, smoke, extend]</td>\n",
       "      <td>pub target curb smoke extend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>Papers reveal secret links with O'Neill cabinet</td>\n",
       "      <td>Papers reveal secret links with O'Neill cabinet</td>\n",
       "      <td>papers reveal secret links with o'neill cabinet</td>\n",
       "      <td>papers reveal secret links with oneill cabinet</td>\n",
       "      <td>[papers, reveal, secret, links, with, oneill, ...</td>\n",
       "      <td>[papers, reveal, secret, links, oneill, cabinet]</td>\n",
       "      <td>[paper, reveal, secret, link, oneil, cabinet]</td>\n",
       "      <td>paper reveal secret link oneil cabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>Domestic chaos as Italy takes EU presidency</td>\n",
       "      <td>Domestic chaos as Italy takes EU presidency</td>\n",
       "      <td>domestic chaos as italy takes eu presidency</td>\n",
       "      <td>domestic chaos as italy takes eu presidency</td>\n",
       "      <td>[domestic, chaos, as, italy, takes, eu, presid...</td>\n",
       "      <td>[domestic, chaos, italy, takes, eu, presidency]</td>\n",
       "      <td>[domest, chao, itali, take, eu, presid]</td>\n",
       "      <td>domest chao itali take eu presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>Learning about the star to which we owe life</td>\n",
       "      <td>Learning about the star to which we owe life</td>\n",
       "      <td>learning about the star to which we owe life</td>\n",
       "      <td>learning about the star to which we owe life</td>\n",
       "      <td>[learning, about, the, star, to, which, we, ow...</td>\n",
       "      <td>[learning, star, owe, life]</td>\n",
       "      <td>[learn, star, owe, life]</td>\n",
       "      <td>learn star owe life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  headline_category                                    headline_text  \\\n",
       "0              news         UUP sees possibility of voting Major out   \n",
       "1              news   Pubs targeted as curbs on smoking are extended   \n",
       "2              news  Papers reveal secret links with O'Neill cabinet   \n",
       "3              news      Domestic chaos as Italy takes EU presidency   \n",
       "4              news     Learning about the star to which we owe life   \n",
       "\n",
       "                                      cleaned_text  \\\n",
       "0         UUP sees possibility of voting Major out   \n",
       "1   Pubs targeted as curbs on smoking are extended   \n",
       "2  Papers reveal secret links with O'Neill cabinet   \n",
       "3      Domestic chaos as Italy takes EU presidency   \n",
       "4     Learning about the star to which we owe life   \n",
       "\n",
       "                                   lower_case_text  \\\n",
       "0         uup sees possibility of voting major out   \n",
       "1   pubs targeted as curbs on smoking are extended   \n",
       "2  papers reveal secret links with o'neill cabinet   \n",
       "3      domestic chaos as italy takes eu presidency   \n",
       "4     learning about the star to which we owe life   \n",
       "\n",
       "                                   no_punctuation  \\\n",
       "0        uup sees possibility of voting major out   \n",
       "1  pubs targeted as curbs on smoking are extended   \n",
       "2  papers reveal secret links with oneill cabinet   \n",
       "3     domestic chaos as italy takes eu presidency   \n",
       "4    learning about the star to which we owe life   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0   [uup, sees, possibility, of, voting, major, out]   \n",
       "1  [pubs, targeted, as, curbs, on, smoking, are, ...   \n",
       "2  [papers, reveal, secret, links, with, oneill, ...   \n",
       "3  [domestic, chaos, as, italy, takes, eu, presid...   \n",
       "4  [learning, about, the, star, to, which, we, ow...   \n",
       "\n",
       "                                  removed_stopwords  \\\n",
       "0           [uup, sees, possibility, voting, major]   \n",
       "1        [pubs, targeted, curbs, smoking, extended]   \n",
       "2  [papers, reveal, secret, links, oneill, cabinet]   \n",
       "3   [domestic, chaos, italy, takes, eu, presidency]   \n",
       "4                       [learning, star, owe, life]   \n",
       "\n",
       "                                   stemmed_words  \\\n",
       "0               [uup, see, possibl, vote, major]   \n",
       "1             [pub, target, curb, smoke, extend]   \n",
       "2  [paper, reveal, secret, link, oneil, cabinet]   \n",
       "3        [domest, chao, itali, take, eu, presid]   \n",
       "4                       [learn, star, owe, life]   \n",
       "\n",
       "                             final_format  \n",
       "0              uup see possibl vote major  \n",
       "1            pub target curb smoke extend  \n",
       "2  paper reveal secret link oneil cabinet  \n",
       "3        domest chao itali take eu presid  \n",
       "4                     learn star owe life  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer()\n",
    "x = tf.fit_transform(news_set['final_format'])\n",
    "y = news_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['news', 'news', 'sport.rugby.international', ..., 'sport', 'sport',\n",
       "       'news'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_1 = classifier.predict(x_test)\n",
    "y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.22914923734588"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score_1 = accuracy_score(y_test, y_pred_1)\n",
    "score_1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['news', 'news', 'sport', ..., 'sport', 'news', 'news'],\n",
       "      dtype='<U57')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_2 = nb.predict(x_test)\n",
    "y_pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.65944077913918"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_2 = accuracy_score(y_test, y_pred_2)\n",
    "score_2*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x_1 = cv.fit_transform(news_set['final_format'])\n",
    "y = news_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_1,y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_1 = LogisticRegression(random_state = 0)\n",
    "classifier_1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3 = classifier_1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.201001104886565"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_3 = accuracy_score(y_test, y_pred_3)\n",
    "score_3*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "nb_1 = MultinomialNB()\n",
    "nb_1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_4 = nb_1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.83294679953545"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_4 = accuracy_score(y_test, y_pred_4)\n",
    "score_4*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best classifier I have for the data  is the count vectorizer logistic regression classifer which got a 57.201 percent accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
